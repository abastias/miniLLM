# ğŸ§  Mini LLM

Build your own GPT-style language model (LLM) from scratch using PyTorch.  
This project is designed to work efficiently on a Mac M CPU or any other machine with modest resources.

---

## ğŸš€ Features

- âœ… Train your own tokenizer (BPE)
- âœ… Implement a decoder-only Transformer (GPT-style)
- âœ… Generate text using top-k sampling
- âœ… Interact with your model using a command-line chat interface
- âœ… Run on Apple Silicon (`mps`) or CPU

---

## ğŸ—‚ï¸ Project Structure

```
mini_llm/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ fundamentals.md
â”œâ”€â”€ train.py
â”œâ”€â”€ generate.py
â”œâ”€â”€ chat.py
â”œâ”€â”€ model/
â”‚   â””â”€â”€ minigpt.py
â”œâ”€â”€ tokenizer/
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ merges.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ your_data.txt      â† Your training dataset
```

---

## ğŸ“¥ Dataset Required

Before training, you must add your own dataset in plain text format.

I recommend starting with:

```bash
curl -o data/your_data.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
```

Or you can use any `.txt` file of your choosing. The better and more varied the text, the smarter your model.

---

## âš™ï¸ Setup Instructions

### 1. Create Python Environment

```bash
python3.11 -m venv myllm-env
source myllm-env/bin/activate
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ‹ï¸â€â™‚ï¸ Train the Model

```bash
python train.py
```

This will:
- Train a tokenizer using your dataset
- Train the transformer model
- Save weights to `minillm.pt`

---

## âœï¸ Generate Text

```bash
python generate.py
```

Change the prompt inside the file or modify the function to generate different samples.

---

## ğŸ’¬ Chat with Your Model

```bash
python chat.py --prompt "Tell me a joke"
```

Or start an interactive loop:

```bash
python chat.py
```

Options:
- `--temperature` (default: 1.0)
- `--top_k` (default: 20)
- `--tokens` (default: 50)

---

## ğŸ§ª Summary

This repo provides a compact and educational GPT-style model pipeline: from tokenizer, to training, to sampling and chat interaction.

Built with â¤ï¸ to demystify LLMs.


by Alfonso G. Bastias, Ph.D.

